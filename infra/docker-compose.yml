services:
  mariadb:
    image: mariadb:11
    environment:
      MARIADB_ROOT_PASSWORD: exemple_root
      MARIADB_DATABASE: rag
      MARIADB_USER: rag_user
      MARIADB_PASSWORD: changeme
    volumes:
      - mariadb_data:/var/lib/mysql
    networks:
      - rag-net

  keycloak:
    image: quay.io/keycloak/keycloak:24.0
    command: start-dev --import-realm
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_HTTP_RELATIVE_PATH: /
    volumes:
      - ./keycloak/realm-export:/opt/keycloak/data/import
    networks:
      - rag-net
    depends_on:
      - mariadb

  qdrant:
    image: qdrant/qdrant:v1.15.1
    ports:
      - "8130:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - rag-net

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "8120:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    networks:
      - rag-net

  vllm:
    image: vllm/vllm-openai:latest
    command:
      - --model
      - ${VLLM_MODEL_NAME-mistralai/Mistral-7B-Instruct-v0.3}
      - --max-model-len
      - ${VLLM_MAX_MODEL_LEN-4096}
      - --gpu-memory-utilization
      - ${VLLM_GPU_MEMORY_UTILIZATION-0.9}
      - --served-model-name
      - ${VLLM_SERVED_MODEL_NAME:-mistral}
      - --host
      - 0.0.0.0
      - --port
      - "8000"
    ports:
      - "8100:8000"
    environment:
      HF_TOKEN: ${HF_TOKEN-}
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN-}
      VLLM_DOWNLOAD_DIR: /models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    volumes:
      - mistral_models:/models
    networks:
      - rag-net
    depends_on:
      - qdrant
    profiles: [ "mistral" ]

  vllm-light:
    image: vllm/vllm-openai:latest
    command:
      - --model
      - ${VLLM_SMALL_MODEL_NAME-microsoft/Phi-3-mini-4k-instruct}
      - --max-model-len
      - ${VLLM_SMALL_MAX_MODEL_LEN-4096}
      - --gpu-memory-utilization
      - ${VLLM_SMALL_GPU_MEMORY_UTILIZATION-0.7}
      - --served-model-name
      - ${VLLM_SMALL_SERVED_MODEL_NAME:-phi3-mini}
      - --host
      - 0.0.0.0
      - --port
      - "8002"
    ports:
      - "8110:8002"
    environment:
      HF_TOKEN: ${HF_TOKEN-}
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN-}
      VLLM_DOWNLOAD_DIR: /models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    volumes:
      - small_models:/models
    networks:
      - rag-net
    depends_on:
      - qdrant
    profiles: [ "light" ]

  ingestion:
    build:
      context: ../ingestion
      dockerfile: Dockerfile
    environment:
      MARIADB_PASSWORD: changeme
    volumes:
      - ../data/examples:/data:ro
    networks:
      - rag-net
    depends_on:
      - mariadb
      - qdrant
    profiles: [ "tools" ]

  insights:
    build:
      context: ../ingestion
      dockerfile: Dockerfile
    entrypoint: [ "python", "-m", "ingestion.insights_cli" ]
    environment:
      MARIADB_PASSWORD: ${MARIADB_PASSWORD:-changeme}
    volumes:
      - ../data/examples:/data:ro
    networks:
      - rag-net
    depends_on:
      - mariadb
    profiles: [ "tools" ]

  inventory:
    build:
      context: ../ingestion
      dockerfile: Dockerfile
    entrypoint: [ "python", "-m", "ingestion.inventory_cli" ]
    environment:
      MARIADB_PASSWORD: ${MARIADB_PASSWORD:-changeme}
    volumes:
      - ../data/examples:/data:ro
    networks:
      - rag-net
    depends_on:
      - mariadb
    profiles: [ "tools" ]

  classification:
    build:
      context: ../ingestion
      dockerfile: Dockerfile
    entrypoint: [ "python", "-m", "ingestion.classify_cli" ]
    environment:
      MARIADB_PASSWORD: ${MARIADB_PASSWORD:-changeme}
      CLASSIFIER_API_BASE: ${CLASSIFIER_API_BASE:-http://vllm:8000/v1}
      CLASSIFIER_API_KEY: ${CLASSIFIER_API_KEY:-changeme}
      CLASSIFIER_MODEL_ID: ${CLASSIFIER_MODEL_ID:-mistral}
      CLASSIFIER_MAX_DOC_CHARS: ${CLASSIFIER_MAX_DOC_CHARS:-6000}
    volumes:
      - ../data/examples:/data:ro
    networks:
      - rag-net
    depends_on:
      - mariadb
    profiles: [ "tools" ]

  indexation:
    build:
      context: ..
      dockerfile: indexation/Dockerfile
    environment:
      QDRANT_URL: http://qdrant:6333
      HF_EMBEDDING_MODEL: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
      ELASTIC_HOST: http://elasticsearch:9200
    volumes:
      - ../data/examples:/data:ro
    networks:
      - rag-net
    depends_on:
      - qdrant
      - elasticsearch
    profiles: [ "tools" ]

  gateway:
    build:
      context: ../llm_pipeline
      dockerfile: Dockerfile
    ports:
      - "8090:8081"
    volumes:
      - ../data/examples:/data:ro
      - ../llm_pipeline:/app/llm_pipeline
    environment:
      PUBLIC_GATEWAY_URL: http://localhost:8090
      VLLM_ENDPOINT: ${VLLM_ENDPOINT:-http://vllm:8000/v1}
      RAG_MODEL_ID: ${RAG_MODEL_ID:-mistral}
      ENABLE_SMALL_MODEL: ${ENABLE_SMALL_MODEL:-false}
      SMALL_MODEL_ID: ${SMALL_MODEL_ID:-phi3-mini}
      SMALL_LLM_ENDPOINT: ${SMALL_LLM_ENDPOINT:-http://vllm-light:8002/v1}
      RAG_TOP_K: ${RAG_TOP_K:-6}
      SMALL_MODEL_TOP_K: ${SMALL_MODEL_TOP_K:-3}
      RAG_MAX_CHUNK_CHARS: ${RAG_MAX_CHUNK_CHARS:-800}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE:-0.0}
      LLM_TIMEOUT: ${LLM_TIMEOUT:-300}
      LLM_MAX_RETRIES: ${LLM_MAX_RETRIES:-2}
      QDRANT_URL: http://qdrant:6333
      ELASTIC_HOST: ${ELASTIC_HOST:-http://elasticsearch:9200}
      ELASTIC_INDEX: ${ELASTIC_INDEX:-rag_documents}
      HYBRID_FUSION: ${HYBRID_FUSION:-rrf}
      HYBRID_WEIGHT_VECTOR: ${HYBRID_WEIGHT_VECTOR:-0.6}
      HYBRID_WEIGHT_KEYWORD: ${HYBRID_WEIGHT_KEYWORD:-0.4}
      HYBRID_BM25_TOP_K: ${HYBRID_BM25_TOP_K:-10}
      KEYCLOAK_URL: http://keycloak:8080/
      HF_EMBEDDING_MODEL: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
      BYPASS_AUTH: "true"
      ENABLE_RERANKER: ${ENABLE_RERANKER:-true}
      DEFAULT_USE_RAG: ${DEFAULT_USE_RAG:-false}
      MARIADB_HOST: ${MARIADB_HOST:-mariadb}
      MARIADB_PORT: ${MARIADB_PORT:-3306}
      MARIADB_DB: ${MARIADB_DB:-rag}
      MARIADB_USER: ${MARIADB_USER:-rag_user}
      MARIADB_PASSWORD: ${MARIADB_PASSWORD:-changeme}
      DATA_ROOT: /data
      ENABLE_INSIGHTS: ${ENABLE_INSIGHTS:-true}
      ENABLE_INVENTORY: ${ENABLE_INVENTORY:-true}
    networks:
      - rag-net
    depends_on:
      - qdrant
      - elasticsearch
      - keycloak

  openwebui:
    build:
      context: ../open-webui
      dockerfile: Dockerfile
      args:
        OLLAMA_BASE_URL: '/ollama'
    ports:
      - "8080:8080"
    volumes:
      - ../openwebui_data:/app/backend/data
    environment:
      WEBUI_SECRET_KEY: ${WEBUI_SECRET_KEY:-change-me}
      OAUTH_PROVIDER: none
      API_BASE_URL: http://gateway:8081/v1
      ENABLE_SIGNUP: "true"
      CORS_ALLOW_ORIGIN: 'http://localhost:8080;http://localhost:5120;http://localhost:5700;http://127.0.0.1:5700;http://localhost:5720'
      # Timeouts pour éviter déconnexions WebSocket pendant requêtes RAG longues
      WEBUI_TIMEOUT: "300"
      OLLAMA_REQUEST_TIMEOUT: "300"
      AIOHTTP_CLIENT_TIMEOUT: "300"
      # Désactiver Ollama pour éviter les erreurs 500 sur /ollama/api/version
      ENABLE_OLLAMA_API: "false"

    networks:
      - rag-net
    depends_on:
      - gateway
      - keycloak

volumes:
  mariadb_data:
  qdrant_data:
  mistral_models:
  esdata:
  small_models:


networks:
  rag-net:
    driver: bridge
